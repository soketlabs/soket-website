<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="description" content="Soket Labs is an AI research firm with a vision to further the advancement in AI towards ethical AGI. We are beginning this journey by build an indigenous large language model trained on Indic languages."/><link rel="preload" as="image" href="/_next/static/media/github-mark.3b925f14.svg" fetchpriority="high"/><link rel="preload" as="image" href="/_next/static/media/hf-logo.e5de6149.svg" fetchpriority="high"/><link rel="preload" as="image" href="/_next/static/media/logo.063f9f62.svg" fetchpriority="high"/><title>Introducing Pragna-1B: Soket AI Labs&#x27; Multilingual Language Model for Indian Languages</title><meta name="next-head-count" content="7"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/b80ad6a2d000972d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b80ad6a2d000972d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/d1c76f4506ef2579.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d1c76f4506ef2579.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-b704c5263bb49c2d.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7d7777a4ca80e423.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/pragna_1b-2417ac57716c2ff3.js" defer=""></script><script src="/_next/static/Jw6tWzuYYOG6O_G0zVkyb/_buildManifest.js" defer=""></script><script src="/_next/static/Jw6tWzuYYOG6O_G0zVkyb/_ssgManifest.js" defer=""></script></head><body><div id="__next"><section class="Layout_layout_section__5w63q"><section class="Header_header_container__RzS4H"><a class="Header_primary_logo_container__c19bj" href="/"><img alt="Soket Labs Logo" fetchpriority="high" width="1211" height="398" decoding="async" data-nimg="1" style="color:transparent;height:70px;width:150px" src="/_next/static/media/logo.063f9f62.svg"/></a><nav><ul><a href="/company"><li>company</li></a><a href="/blogs/bhasha_wiki"><li>blog</li></a><a href="/llm"><li>llm</li></a><a href="/ethics"><li>ethics</li></a><a href="/research"><li>research</li></a><a href="/team"><li>team</li></a><a href="/contact"><li>contact</li></a></ul><div class="Header_social_container__LJbNb"><a target="_blank" href="https://github.com/soketlabs"><img alt="Github Logo" fetchpriority="high" width="98" height="96" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/github-mark.3b925f14.svg"/></a><a target="_blank" href="https://huggingface.co/soketlabs"><img alt="HuggingFace Logo" fetchpriority="high" width="192" height="178" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/hf-logo.e5de6149.svg"/></a></div></nav><a class="Header_secondary_logo_container__wMHMT" href="/"><img alt="Soket Labs Logo" fetchpriority="high" width="1211" height="398" decoding="async" data-nimg="1" style="color:transparent;height:70px;width:150px" src="/_next/static/media/logo.063f9f62.svg"/></a></section>
<div class="Blogs_Blogs__7qjHU"><p><code>28th April, 2024</code></p><h1>Introducing Pragna-1B: Soket AI Labs&#x27; Multilingual Language Model for Indian Languages</h1><p><img src="/images/blog_bhasha_wiki/cover.png" alt="pragna-1b on huggingface"/></p><h2>Introduction: Unleashing the Power of Multilingual AI</h2><p>We at Soket AI Labs are thrilled to unveil our first open source multilingual language model, Pragna-1B available in four Indian languages - Hindi, Gujarati, Bangla and English. The model is designed to cater to the rich tapestry of Indian languages, significantly expanding the horizons of AI inclusivity and accessibility. As we step into an era where technology transcends linguistic boundaries, Pragna-1B emerges as a beacon of innovation, ready to bridge language barriers and enhance user engagement across diverse linguistic landscapes.</p><h2>Highlights of Pragna-1B:</h2><ul>
<li><strong>High-Performance Architecture:</strong> Pragna-1B features a Transformer Encoder-only model with 1.25 billion parameters, optimised for a context length of 2048 tokens.</li>
<li><strong>Designed for Edge AI:</strong> Engineered to deliver state-of-the-art performance in the smallest form factor, ideal for deployment on edge devices.</li>
<li><strong>Small Language Model (SLM) with Robust Capabilities:</strong> Despite its classification as an SLM, Pragna-1B&#x27;s performance rivals larger models, offering comprehensive multilingual support for English, Hindi, Bengali, and Gujarati.</li>
<li><strong>Culturally Contextualised Training:</strong> The model has been meticulously trained on curated datasets specifically designed to reflect the Indian context, ensuring accurate and culturally relevant outputs.</li>
<li><strong>Ethical and Responsible AI:</strong> Committed to upholding human values, the model integrates mechanisms to ensure ethical alignment in its responses.</li>
<li><strong>Open Source Availability:</strong> The base version of Pragna-1B is accessible as an open-source model on Hugging Face, facilitating widespread use and collaboration.</li>
</ul><h2>Architecture Overview</h2><p>Pragna-1B is a decoder-only transformer model inspired by TinyLlama, featuring the following specifications:</p><ul>
<li><strong>Layers:</strong> 22</li>
<li><strong>Attention Heads:</strong> 32</li>
<li><strong>Context Length:</strong> 2048</li>
<li><strong>Hidden Dimension:</strong> 2048</li>
<li><strong>Expansion Dimension:</strong> 5632</li>
<li><strong>Vocabulary Size:</strong> 69632</li>
</ul><p>This model incorporates Rotary Positional Encoding to infuse positional information into the embeddings, utilising a base of 10,000. It employs RSNorm with an epsilon value of 1e-5 and the Sigmoid Activation Unit (SiLU) as the activation function. Additionally, Pragna-1B adopts Grouped Query Attention, an alternative to full attention, which enhances training and inference speed while reducing memory bandwidth. This adjustment also supports the use of lower-compute devices for inference tasks.</p><p>Pragna-1B is trained on a proprietary platform known as GenAI Studio, a modular AI Developer Platform designed to support any GenAI model architecture. It is capable of scaling across thousands of GPUs or accelerators and is built to be fault-tolerant. The development of this model leveraged Triton, an open-source language from OpenAI, for crafting high-performance custom fused CUDA Kernels for most operations. Furthermore, the model uses Fully Sharded Data Parallel (FSDP) for distributed and parallel training and incorporates the state-of-the-art FlashAttention2 technology to accelerate training and inference processes.</p><h2>Developing an Efficient Tokenizer for Indian Languages</h2><p>Pragna employs a Byte-Pair Encoding (BPE) tokenizer, specifically engineered for handling Indian languages. The tokenizer was meticulously trained on six Indian languages—Hindi, Bengali, Urdu, Tamil, Kannada, and Gujarati—before combining them into a unified tokenizer. This unification was achieved through a union set operation, which ensures each language is represented equitably. The merging rules were carefully designed to optimise the compression of bytes into tokens, effectively making the vocabulary size 69,632.</p><p>The tokenizer demonstrates its efficiency through its fertility score, a metric that measures the average number of tokens produced per word. This score is crucial as it reflects the tokenizer&#x27;s ability to compress text into tokens—computer representable integers—with a lower score indicating more efficient tokenization. Notably, our tokenizer not only performs comparably to those designed for English but excels in processing Hindi, Bengali, Gujarati, Tamil, Kannada, and Urdu. For example, the Gemma-7b model produces 5.8 tokens per word whereas Pragna-1b generates 2.8 tokens per word for Kannada almost doubling the throughput just with tokenization.</p><p>Moreover, our tokenizer addresses a significant issue prevalent in existing models: the inadequate representation of Indic languages. Traditional models often dissect Indic characters into bytes, severely hindering performance and requiring an excessive number of tokens for effective training. In contrast, our approach significantly enhances model efficiency and performance across Indian languages.</p><p><img src="/images/blog_pragna/tok_fertility.png" alt="pragna-1b tokenizer"/></p><p>Graph above shows the fertility score for six Indian languages when tokenized using various LLMs.</p><h2>Training Data</h2><p>The quality and quantity of training data are critical in imparting linguistic and semantic understanding into any language model. One significant challenge in developing language models for Indian languages is the scarcity of large-scale data corpora. With all 22 scheduled Indian languages representing less than 1% of internet-scale datasets like MC4, we were compelled to create &quot;Bhasha,&quot; a series of high-quality datasets designed for pretraining and instruction fine-tuning of Indian models.</p><p><strong>Bhasha-wiki:</strong> This dataset comprises 44.1 million Wikipedia articles translated into six major Indian languages from 6.3 million English articles. With over 45.1 billion Indic tokens, Bhasha-wiki serves as a foundational resource for linguistic and AI research. It supports a broad spectrum of applications, including machine translation, natural language processing, and language model training.</p><p>Bhasha-wiki-indic: A refined subset of the Bhasha-wiki, Bhasha-wiki-indic is curated to enrich models with an in-depth understanding of the Indian context. This subset specifically includes content of significant relevance to India, aiming to develop culturally resonant AI applications.</p><p>Bhasha-SFT: Designed to facilitate the development of language models capable of handling various NLP tasks such as multi-turn conversation, question-answering, text summarization, context-based Q&amp;A, and natural language generation, the Bhasha-SFT dataset provides essential tools for enhancing language model performance across diverse linguistic landscapes.</p><p>We have also incorporated external datasets like SlimPajama, a clean and deduplicated dataset by Cerebras featuring 627 billion tokens with a majority in English, and Sangraha-Verified by AI4Bharat, a 15 million tuple dataset in multiple Indian languages, curated from human-verified sources.</p><p>Both the quality and quantity of training data are paramount in the construction of large language models (LLMs). Our training approach involved phased model development to ensure language alignment, infuse the model with extensive knowledge, and embed responsible and ethical nuances.</p><h2>Training Procedure</h2><p><strong>Model Weight Initialization:</strong> Instead of starting from a random distribution, we initialise the weights using TinyLlama-1.1B-Chat-v1.0, a model under the Apache-2.0 licence. This approach leverages the foundational knowledge of an existing model, reducing costs and facilitating the transfer of learning from the English domain to the Indian language domain.</p><p><strong>Indic Tokenizer:</strong> Our base tokenizer, borrowed from Llama-2, initially had a vocabulary size of 32,000. To this, we added new tokens for six Indian languages, expanding the vocabulary to 69,632. This enhancement significantly improved the fertility score across all six languages.</p><p><strong>Embedding Initialization for New Tokens:</strong> Drawing inspiration from John Hewitt’s research, &quot;Initializing New Word Embeddings for Pretrained Language Models,&quot; we opted for a targeted approach to initialising embeddings for new tokens. Instead of averaging all existing embeddings with added noise, we used a subset of relevant embeddings. This method assumes each new Indic token can be tokenized using Llama-2&#x27;s existing vocabulary. By averaging these related embedding vectors, we achieved a more accurate representation with lower KL-divergence. We are preparing a detailed blog post to share our methodology and findings on efficient vocabulary expansion.</p><p><strong>Embedding Alignment:</strong> Initially, we aligned only the embedding and lm_head, keeping other tensors frozen. We utilised a parallel sentences dataset from Bhasha-wiki, pairing sentences in one of the six Indian languages with their English counterparts. Loss was computed solely on the English sentences, aiding in the alignment of model embeddings and facilitating the generation of coherent yet nonsensical text in all six languages with minimal computational demand.</p><p><strong>Continual Pretraining:</strong> We subsequently enabled all 1.25 billion parameters for further training, focusing on three languages—Hindi, Bangla, and Gujarati—due to compute limitations. We maintained high, equal sampling probabilities for these languages while incorporating data from SlimPajama, predominantly in English. The model processed approximately 150 billion tokens (3.15 trillion considering the base model) over about 8,000 GPU-hours. Training specifics included 3,000 hours on 4xA100-40 GiB and 5,000 hours on 8xA100-80 GiB systems.</p><p><strong>Training Parameters:</strong></p><ul>
<li><strong>Precision:</strong> Bfloat16</li>
<li><strong>Batch Size:</strong> 2k - 2.5k</li>
<li><strong>Context Length:</strong> 2,048</li>
<li><strong>Learning Rate:</strong> 3e-5</li>
<li><strong>Optimizer:</strong> AdamW</li>
<li><strong>LR Scheduler:</strong> Cosine</li>
<li><strong>Mixed Precision Training</strong></li>
</ul><p><strong>Instruction Fine-Tuning:</strong> We performed supervised fine-tuning across multiple tasks, including conversation, question-answering, summarization, and paraphrasing, using datasets such as Aya Collection, Indic-align, and Samvaad from Bhasha-SFT, which comprises over 13 million instances of instruction-response data in three Indian languages.</p><p><strong>Ethics and Safety Alignment:</strong> We have ensured the exclusion of toxic and biassed content in our training data. Additionally, we developed our own datasets specifically curated to prevent the model from generating unethical or potentially harmful content.</p><p><strong>Ongoing Developments and Community Engagement:</strong> The model is currently undergoing further instruction tuning and safety alignment. We plan to release the pragna-1b-it model weights to the community under the same open-source licence soon. An initial research preview of the instruction-tuned model is available via a chat interface, though it is not recommended for production use due to its tendency to generate convincing yet factually incorrect information. We eagerly anticipate feedback from the community to further refine and enhance our models.</p><h2>Inference</h2><h3>Next Token Prediction</h3><div class="Blogs_sample_table__pp3Fe"><table><thead><tr><th>Context</th><th>Tokens Predicted</th></tr></thead><tbody><tr><td>India is one of the most</td><td>India is one of the most exciting places to travel to in Asia. The country hosts a wide variety of cultures and offers a plethora of attractions across the country. There&#x27;s a lot you can do here and the tourists are always on the lookout for the new things to do and see. If you&#x27;re looking to travel to India for your next adventure, you&#x27;ll be pleased to know that you can find a ton of things to do in the country. From the temples to the beaches, there&#x27;s a lot you can see and do here. One of the best things for visitors to do in India is to visit Jaipur. The capital city of Rajputana, Jaipur is a beautiful city that&#x27;s known as the &quot;City of Lives&quot;.</td></tr><tr><td>नरेंद्र मोदी का जन्म</td><td>नरेंद्र मोदी का जन्मदिन आज मनाया जा रहा है। इस मौके पर प्रधानमंत्री नरेंद्र मोदी ने ट्वीट कर लिखा कि &#x27;आज देश को एक नया संसद भवन मिला है, जिसे आप लोगों ने बधाई दी है। भारत और भारतवासियों को यह संसद बहुत ही महत्त्वपूर्ण है और इसका उपयोग देश के विकास में बहुत अधिक योगदान देगा। आज हमारे देश में एक संसद बनी हुई है जो हमारे देश की लोकतांत्रिक प्रक्रियाओं को मजबूत बनाती है।</td></tr><tr><td>Prime minister Modi</td><td>Prime minister Modi&#x27;s visit to China: PM Modi is scheduled to arrive in Beijing on Thursday. PM Narendra Modi will be the first Indian prime minister to visit China since former President Mao Zedong, who visited India in 1959 and was assassinated by Chinese soldiers a year later. The two leaders are expected to discuss regional security issues such as terrorism, trade and energy cooperation during their talks.</td></tr><tr><td>महात्मा गांधी</td><td>महात्मा गांधी एक स्वतंत्र व्यक्ति, एक आत्मनिर्भर और एक अत्यंत लोकप्रिय नेता थे। उन्होंने अपने जीवनकाल में देश को एक नई दिशा देने के लिए बहुत कुछ किया। समाज के विभिन्न क्षेत्रों में उनके योगदान के लिए उन्हें सम्मानित किया गया था।</td></tr><tr><td>ब्रिटिश राजशाही</td><td>ब्रिटिश राजशाही के विघटन के बाद, इंग्लैंड और वेल्स में ब्रिटेन की संसद का गठन हुआ। संसद ने 1807 में अपने पहले अधिनियम, &quot;द क्राउन की विधेयक&quot; (एक्ट ऑफ द डेमोक्रैटिक) को मंजूरी दी, जिसमें प्रत्येक राज्य से एक निर्वाचित सदस्य चुना गया।</td></tr><tr><td>মহাত্মা গান্ধী</td><td>মহাত্মা গান্ধী ছিলেন আন্তর্জাতিক সমাজতান্ত্রিক আন্দোলনের অন্যতম প্রধান নেতা। তিনি ভারতের স্বাধীনতা সংগ্রামে তাঁর ভূমিকা অপরিসীম। ১৯৪৭ সালের ১৫ আগস্ট ভারতে বিপ্লব ঘটে। ১৯৪৮ সালে মহাত্মা গান্ধীর মৃত্যু হয়। এরপর থেকে মহাত্মা আর দেখা যায় না।</td></tr><tr><td>নরেন্দ্র মোদীর জন্ম</td><td>নরেন্দ্র মোদীর জন্মদিনের শুভেচ্ছা জানিয়েছেন অমিতাভ বচ্চন। প্রসঙ্গত, গোটা দেশ জুড়ে চলছে নববর্ষ উৎসব। আর এই উৎসবে নিজেকে সামিল করতে চান না কেউই। এমনটাই জানালেন বলিউড অভিনেত্রী সারা আলি খান।</td></tr><tr><td>એક વખત</td><td>ક વખતે, તેમણે તેમના મિત્ર અને સહાયક વિલિયમ હેનરી મેકડોનાલ્ડ સાથે મળીને &quot;ધ ઓરિજિનલ મેન ઓફ ધ ડેડ&quot; નામના પુસ્તકનું નિર્માણ કર્યું હતું; તે 1961માં પ્રકાશિત થયું હતું. 1958માં, તેમણે ધ ન્યૂ યોર્કર માટે લેખક અને સંપાદક પિતા તરીકે કામ કર્યું હતું.</td></tr><tr><td>એક સમયે એક રાજા જંગલમાં રહેતો હતો. તેમણે</td><td>એક સમયે એક રાજા જંગલમાં રહેતો હતો. તેમણે, એક દિવસ રાજાને કહ્યું, &quot;હું તમને મારી જાતને એક સ્ત્રી તરીકે આપું છું.&quot; રાજા ખુશ થયો. તેમણે તેને તેના ઘરમાં લઈ ગયા. તે દિવસે રાણી પોતાના પતિ સાથે ઘરે આવી અને કહ્યું કે તે તેની પાસે છે. રાજાએ તેને પૂછ્યું કે શું તેણે તેની સાથે લગ્ન કર્યા છે ? રાજા અને રાણીએ પૂછ્યું - હા. ત્યારે તેમણે કહ્યું કે, &quot;તમે મને તમારી સાથે લગ્ન કરવાનું વચન આપ્યું છે. હવે મારે તમારા પર વિશ્વાસ રાખવો જોઈએ નહીં&quot;.</td></tr></tbody></table></div><h3>SFT outputs</h3><div class="Blogs_sample_table__pp3Fe"><table><thead><tr><th>Instruction</th><th>Response</th></tr></thead><tbody><tr><td>Tell me about India</td><td>India is a vast and diverse country located in South Asia. It is the seventh-largest country in the world by land area and is also the most populous country, with over 1.4 billion people as of my last update. Here are some key points about India: <br/><br/>Inchurching History: Modern India has a long and rich history that spans thousands of years. It was under various empires for many centuries before it became part of the British Empire in the 18th century. The Indian Rebellion of 1857 was a significant event that led to the independence of India from British rule and its unification into one nation. <br/><br/>Economy: India&#x27;s economy is one of the largest in the world, with economic growth reportedly averaging around 7% per year since 1990. It is considered one of the most advanced economies in the world and hosts several multinational companies, including Tata Motors, Airtel, HCL Teletracns, and Infosys among others. <br/><br/>Society: India&#x27;s society is diverse and pluralistic, with individuals belonging to different religions, ethnicies, and social classes. It faces challenges such as poverty, inequality, and environmental degradation.</td></tr><tr><td>मुझे भारत के बारे में बताओ?</td><td>भारत दक्षिण एशिया में स्थित एक विशाल और वैश्विक देश है। यह भूमि क्षेत्र के हिसाब से दुनिया का सातवां सबसे बड़ा देश है और सबसे अधिक आबादी वाला देश है, जहां मेरी पिछली अद्यतन जानकारी के अनुसार 12.2 करोड़ लोग हैं। वे हिंदू समुदाय का नेतृत्व करते हैं, और एक संघीय संसदीय लोकतांत्रिक गणराज्य है।</td></tr><tr><td>भारत के प्रधान मंत्री कौन हैं</td><td>नरेंद्र मोदी भारत के वर्तमान प्रधानमंत्री हैं।</td></tr><tr><td>भारत को आजादी कब मिली</td><td>1942 में भारत के स्वतंत्रता संग्राम के दौरान भारत ने अपनी सार्वजनिक रूप से घोषित स्वतंत्रता के लिए लड़ाई लड़ी थी। यह युद्ध 1947 तक चला, तब भारत की स्थापना हुई और इसे भारतीय संविधान द्वारा मान्यता प्राप्त हुई।</td></tr><tr><td>আমাকে ভারত সম্পর্কে বলুন</td><td>ভারত একটি বিশাল এবং বৈচিত্র্যময় দেশ যা বৈচিত্র্যময় সাংস্কৃতিক ও ধর্মীয় দৃষ্টিভঙ্গি প্রদান করে। এটি বিভিন্ন ভাষার কেন্দ্রীভূত কাঠামো এবং বিভিন্ন ধর্মীয় পদ্ধতির জন্য পরিচিত।</td></tr><tr><td>বৈজ্ঞানিক অর্জন মানে কি?</td><td>বৈজ্ঞানিক অর্জনগুলি গণনা, যোগাযোগ এবং বৈজ্ঞানিক সংস্থার মাধ্যমে বা ব্যক্তিগতভাবে অর্জিত বিভিন্ন অর্জন হতে পারে।</td></tr></tbody></table></div><h2>Model Evaluation and Benchmarks</h2><div class="Blogs_bench_table__rOvgZ"><h3>Hindi</h3><table><thead><tr><th></th><th>Arc-Easy</th><th>Arc-Challenge</th><th>Hellaswag</th><th>Average</th></tr></thead><tbody><tr><td>pragna-1b</td><td>0.33</td><td>0.22</td><td>0.35</td><td>0.30</td></tr><tr><td>sarvamai/OpenHathi-7B-Hi-v0.1-Base</td><td>0.3582</td><td>0.2645</td><td>0.4315</td><td>0.35</td></tr><tr><td>meta-llama/Llama-2-7b-hf</td><td>0.295</td><td>0.2406</td><td>0.3789</td><td>0.30</td></tr><tr><td>google/gemma-7b</td><td>0.5926</td><td>0.4258</td><td>0.6341</td><td>0.55</td></tr><tr><td>meta-llama/Meta-Llama-3-8B</td><td>0.5354</td><td>0.3541</td><td>0.6072</td><td>0.50</td></tr></tbody></table><h3>Gujarati</h3><table><thead><tr><th></th><th>Arc-Easy</th><th>Arc-Challenge</th><th>Hellaswag</th><th>Average</th></tr></thead><tbody><tr><td>pragna-1b</td><td>0.32</td><td>0.22</td><td>0.37</td><td>0.30</td></tr><tr><td>google/gemma-7b</td><td>0.4954</td><td>0.3208</td><td>0.5673</td><td>0.46</td></tr></tbody></table><h3>English</h3><table><thead><tr><th></th><th>Arc</th><th>Hellaswag</th><th>MMLU</th><th>TruthfulQA</th><th>Winogrande</th><th>GSM8K</th><th>Average</th></tr></thead><tbody><tr><td>pragna-1b</td><td>0.3</td><td>0.51</td><td>0.27</td><td>0.38</td><td>0.56</td><td>0</td><td>0.34</td></tr><tr><td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td><td>0.36</td><td>0.61</td><td>0.25</td><td>0.37</td><td>0.61</td><td>0.02</td><td>0.37</td></tr><tr><td>meta-llama/Meta-Llama-3-8B-Instruct</td><td>0.6</td><td>0.78</td><td>0.67</td><td>0.51</td><td>0.74</td><td>0.68</td><td>0.66</td></tr><tr><td>meta-llama/Llama-2-7b-hf</td><td>0.53</td><td>0.78</td><td>0.46</td><td>0.39</td><td>0.74</td><td>0.14</td><td>0.51</td></tr></tbody></table></div><h4>Eval numbers for Hindi and Gujarati are taken from Indic LLM Leaderboard and English from HF&#x27;s Open LLM Leaderboard</h4><p>Being a 1.25 billion parameter model, its performance is comparable to models of 7 billion category.</p></div><section class="Footer_footer_section__0aags"><small>Soket Labs Technology and Research Private Limited © 2023</small><small><b>Disclaimer: </b>Some content on this website is generated using AI</small></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/blogs/pragna_1b","query":{},"buildId":"Jw6tWzuYYOG6O_G0zVkyb","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>