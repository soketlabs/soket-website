<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="description" content="Soket Labs is an AI research firm with a vision to further the advancement in AI towards ethical AGI. We are beginning this journey by build an indigenous large language model trained on Indic languages."/><link rel="preload" as="image" href="/images/Soket-Logo.svg" fetchpriority="high"/><title>Introducing &quot;Bhasha&quot; Indic Language AI Datasets</title><meta name="next-head-count" content="5"/><link rel="preload" href="/_next/static/css/29b16b7de062c912.css" as="style"/><link rel="stylesheet" href="/_next/static/css/29b16b7de062c912.css" data-n-g=""/><link rel="preload" href="/_next/static/css/15d6909fd2fa045b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/15d6909fd2fa045b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-0b5d8249fb15f5f3.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-b704c5263bb49c2d.js" defer=""></script><script src="/_next/static/chunks/pages/_app-3831540fb6633031.js" defer=""></script><script src="/_next/static/chunks/187-b72fd988ebac0dbe.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/bhasha_wiki-5bdcb51774732b13.js" defer=""></script><script src="/_next/static/-N6ffAJ5LOAVG2GVE7VnU/_buildManifest.js" defer=""></script><script src="/_next/static/-N6ffAJ5LOAVG2GVE7VnU/_ssgManifest.js" defer=""></script></head><body><div id="__next"><section class="Layout_layout_section__5w63q"><header class="Header_header_container__RzS4H"><div class="Header_header_inner__p8cLp"><div class="Header_primary_logo_container__c19bj"><a href="/"><img alt="Soket Labs Logo" fetchpriority="high" width="150" height="70" decoding="async" data-nimg="1" style="color:transparent;height:70px;width:150px" src="/images/Soket-Logo.svg"/></a></div><button class="Header_mobile_menu_button__0ZDcA" aria-label="Toggle menu"><span class="Header_hamburger__IDMFE"></span></button><nav class="undefined "><ul><a href="/blogs/pragna_1b"><li>pragna-1b</li></a><a href="/blogs/bhasha_wiki"><li>bhasha</li></a></ul><a class="inline-flex items-center justify-center px-8 py-4 text-base font-medium font-geist transition-all duration-200 relative bg-black text-white hover:bg-soket-dark" href="mailto:careers@soket.ai"><div class="absolute w-0 h-0 top-0 left-0 border-t-[16px] border-l-[16px] border-l-transparent rotate-[270deg] border-t-white"></div><div class="absolute w-0 h-0 bottom-0 right-0 border-b-[16px] border-l-[16px] border-b-transparent rotate-[540deg] border-l-white"></div>GET IN TOUCH</a></nav></div></header>
<div class="Blogs_Blogs__7qjHU"><p><code>17th April, 2024</code></p><h1>Introducing the &quot;Bhasha&quot; Series ðŸš€: Advancements in Indic Language AI Datasets</h1><p><img src="/images/blog_bhasha_wiki/cover.png" alt="bhasha-wiki on huggingface"/></p><p>Soket Labs is pleased to announce ðŸ¥³ the release of the &quot;Bhasha&quot; series, commencing with two significant datasets: <a href="https://huggingface.co/datasets/soketlabs/bhasha-wiki">&quot;bhasha-wiki&quot;</a> and <a href="https://huggingface.co/datasets/soketlabs/bhasha-wiki-indic">&quot;bhasha-wiki-indic&quot;</a>. These datasets are engineered to support the development of AI models that are attuned to the linguistic and cultural nuances of India ðŸ«¡, representing a crucial step forward in the diversification of linguistic resources in computational linguistics. By making these datasets available in an open-source format, we aim to foster a collaborative environment where developers and researchers across India can contribute to and benefit from inclusive and contextually aware AI technologies</p><p>Stay tuned for exciting updates by following us on <a href="https://www.linkedin.com/company/soketlabs">LinkedIn</a></p><h2>Bhasha-wiki: A Comprehensive Corpus for Indic Language Research</h2><h3>Available on Huggingface ðŸ¤—: <a href="https://huggingface.co/datasets/soketlabs/bhasha-wiki">soketlabs/bhasha-wiki</a></h3><p>The &quot;bhasha-wiki&quot; dataset presents a comprehensive corpus consisting of 44.1 million Wikipedia articles translated into six major Indian languages from 6.3 million English articles. This corpus, encompassing over 45.1 billion Indic tokens, serves as a foundational resource for linguistic and AI research, facilitating a wide range of studies into machine translation, natural language processing, and language model training.</p><h3>Dataset Characteristics:</h3><ul>
<li><strong>Extensive Lexical Volume:</strong> The corpus is substantial, with a total size of 117 GiB, containing 44,418,479 rows and over 20 billion words.</li>
<li><strong>Linguistic Diversity:</strong> This dataset supports a multilingual framework, including Hindi, Gujarati, Urdu, Tamil, Kannada, Bengali, and English, crucial for cross-linguistic studies.</li>
<li><strong>Translation Methodologies:</strong> Utilising IndicTrans2, powered by a significant computational resources (3360 GPU-hours on AWS), each article was translated with high fidelity to the original content. Segmentation and translation were handled sentence-by-sentence, with adaptations made for longer sentences to maintain semantic integrity.</li>
</ul><table><thead><tr><th></th><th>Sentences</th><th>Characters</th><th>Words</th><th>Tokens</th><th>Rows</th></tr></thead><tbody><tr><td>english</td><td>149,636,946</td><td>19,009,297,439</td><td>2,954,105,643</td><td>5,430,358,976</td><td>6,345,497</td></tr><tr><td>hindi</td><td>149,636,946</td><td>18,622,892,252</td><td>3,382,736,074</td><td>6,635,241,630</td><td>6,345,497</td></tr><tr><td>kannada</td><td>149,636,946</td><td>19,679,016,421</td><td>2,349,908,384</td><td>6,083,839,825</td><td>6,345,497</td></tr><tr><td>bengali</td><td>149,636,946</td><td>18,741,174,694</td><td>2,663,832,869</td><td>8,248,287,687</td><td>6,345,497</td></tr><tr><td>gujarati</td><td>149,636,946</td><td>18,453,210,446</td><td>2,867,239,209</td><td>6,032,149,490</td><td>6,345,497</td></tr><tr><td>tamil</td><td>149,636,946</td><td>21,457,803,696</td><td>2,441,061,609</td><td>6,777,927,96</td><td>6,345,497</td></tr><tr><td>urdu</td><td>149,636,946</td><td>17,921,351,051</td><td>3,641,717,085</td><td>5,966,954,204</td><td>6,345,497</td></tr><tr><td>-</td><td>-----------</td><td>------------</td><td>-------</td><td>--------</td><td>------</td></tr><tr><td>Total</td><td>1,04,74,58,622</td><td>133,884,745,999</td><td>20,300,600,873</td><td>45,174,759,774</td><td>44,418,479</td></tr></tbody></table><p><code>Note: Tokens are calculated using pragna-1b tokenizer</code></p><p>Characters, words and token distribution for each language is shown in the image.</p><p><img src="/images/blog_bhasha_wiki/chars_words_tokens_per_language.png" alt="bhasha-wiki on huggingface"/></p><h2>Bhasha-wiki-indic: Tailored Dataset for Enhanced Indian Contextual Relevance</h2><h3>Available on Huggingface: <a href="https://huggingface.co/datasets/soketlabs/bhasha-wiki-indic">soketlabs/bhasha-wiki-indic</a></h3><p>The &quot;bhasha-wiki-indic&quot; dataset, a refined subset of the &quot;bhasha-wiki&quot;, is specifically curated to enrich models with a deeper understanding of the Indian context. This subset was meticulously selected to include content with significant relevance to India, enhancing the potential for developing culturally resonant AI applications.</p><h3>Methodology:</h3><ul>
<li><strong>Focused Semantic Filtering:</strong> Initial filtering employed keyword detection (&#x27;india&#x27; or &#x27;indian&#x27;), refined by a topic classifier, achieving an 84% accuracy in distinguishing relevant content.</li>
<li><strong>Content Extraction and Processing:</strong> Approximately 208,000 articles were identified as contextually relevant and subsequently extracted for six Indian languages, preparing this dataset as a specialised tool for AI models requiring deep cultural comprehension.</li>
</ul><h3>Dataset Specifications:</h3><ul>
<li><strong>Content Volume:</strong> The dataset comprises 200,820 rows with nearly 1.54 billion tokens distributed among several languages, providing a rich linguistic base for detailed computational analysis.</li>
</ul><h2>Contributions to the Field and Future Directions</h2><p>These datasets are expected to significantly impact research in computational linguistics and AI by providing high-quality, large-scale resources for training models that require a nuanced understanding of Indian languages and contexts. They also serve as a platform for further scholarly inquiry into algorithmic translations and cultural specificity in AI technologies.</p><h2>Open Access and Collaborative Engagement</h2><p>Released under the CC-by-SA-3.0 licence, these datasets facilitate both academic and commercial use, promoting a wide dissemination and application in diverse settings. We invite the global research community to engage with these resources, further enriching the datasets and exploring new frontiers in AI research.</p><p>As we continue to develop the &quot;Bhasha&quot; series, we remain committed to advancing the state of AI with a focus on ethical considerations and inclusivity in technology.</p><h2>About Soket Labs:</h2><p>Soket Labs, a visionary AI research firm, is at the forefront of promoting advancements towards ethical Artificial General Intelligence (AGI). Our mission is to foster a form of general intelligence that excels in efficiency and accessibility, thereby democratising cutting-edge technology for diverse applications, including autonomous robots, edge devices, and large clusters.</p></div><footer class="bg-soket-dark py-7"><div class="mx-5 lg:mx-[8%] flex flex-col md:flex-row justify-between items-center"><p class="text-white/80 text-sm lg:text-base font-geist-mono mb-4 md:mb-0 text-center lg:text-left">Â© 2025 SOKET AI LABS. ALL RIGHTS RESERVED</p><div class="flex space-x-6"><a class="text-white/80 hover:text-white transition-colors" target="_blank" rel="noopener noreferrer" href="https://x.com/soketlabs"><span class="sr-only">X</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></a><a class="text-white/80 hover:text-white transition-colors" target="_blank" rel="noopener noreferrer" href="https://www.instagram.com/soketlabs/"><span class="sr-only">Instagram</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12.315 2c2.43 0 2.784.013 3.808.06 1.064.049 1.791.218 2.427.465a4.902 4.902 0 011.772 1.153 4.902 4.902 0 011.153 1.772c.247.636.416 1.363.465 2.427.048 1.067.06 1.407.06 4.123v.08c0 2.643-.012 2.987-.06 4.043-.049 1.064-.218 1.791-.465 2.427a4.902 4.902 0 01-1.153 1.772 4.902 4.902 0 01-1.772 1.153c-.636.247-1.363.416-2.427.465-1.067.048-1.407.06-4.123.06h-.08c-2.643 0-2.987-.012-4.043-.06-1.064-.049-1.791-.218-2.427-.465a4.902 4.902 0 01-1.772-1.153 4.902 4.902 0 01-1.153-1.772c-.247-.636-.416-1.363-.465-2.427-.047-1.024-.06-1.379-.06-3.808v-.63c0-2.43.013-2.784.06-3.808.049-1.064.218-1.791.465-2.427a4.902 4.902 0 011.153-1.772A4.902 4.902 0 015.45 2.525c.636-.247 1.363-.416 2.427-.465C8.901 2.013 9.256 2 11.685 2h.63zm-.081 1.802h-.468c-2.456 0-2.784.011-3.807.058-.975.045-1.504.207-1.857.344-.467.182-.8.398-1.15.748-.35.35-.566.683-.748 1.15-.137.353-.3.882-.344 1.857-.047 1.023-.058 1.351-.058 3.807v.468c0 2.456.011 2.784.058 3.807.045.975.207 1.504.344 1.857.182.466.399.8.748 1.15.35.35.683.566 1.15.748.353.137.882.3 1.857.344 1.054.048 1.37.058 4.041.058h.08c2.597 0 2.917-.01 3.96-.058.976-.045 1.505-.207 1.858-.344.466-.182.8-.398 1.15-.748.35-.35.566-.683.748-1.15.137-.353.3-.882.344-1.857.048-1.055.058-1.37.058-4.041v-.08c0-2.597-.01-2.917-.058-3.96-.045-.976-.207-1.505-.344-1.858a3.097 3.097 0 00-.748-1.15 3.098 3.098 0 00-1.15-.748c-.353-.137-.882-.3-1.857-.344-1.023-.047-1.351-.058-3.807-.058zM12 6.865a5.135 5.135 0 110 10.27 5.135 5.135 0 010-10.27zm0 1.802a3.333 3.333 0 100 6.666 3.333 3.333 0 000-6.666zm5.338-3.205a1.2 1.2 0 110 2.4 1.2 1.2 0 010-2.4z" clip-rule="evenodd"></path></svg></a><a class="text-white/80 hover:text-white transition-colors" target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/@SoketAI"><span class="sr-only">YouTube</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M19.812 5.418c.861.23 1.538.907 1.768 1.768C21.998 8.746 22 12 22 12s0 3.255-.418 4.814a2.504 2.504 0 0 1-1.768 1.768c-1.56.419-7.814.419-7.814.419s-6.255 0-7.814-.419a2.505 2.505 0 0 1-1.768-1.768C2 15.255 2 12 2 12s0-3.255.417-4.814a2.507 2.507 0 0 1 1.768-1.768C5.744 5 11.998 5 11.998 5s6.255 0 7.814.418ZM15.194 12 10 15V9l5.194 3Z" clip-rule="evenodd"></path></svg></a><a class="text-white/80 hover:text-white transition-colors" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/company/soketlabs"><span class="sr-only">LinkedIn</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" clip-rule="evenodd"></path></svg></a></div></div></footer></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/blogs/bhasha_wiki","query":{},"buildId":"-N6ffAJ5LOAVG2GVE7VnU","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>